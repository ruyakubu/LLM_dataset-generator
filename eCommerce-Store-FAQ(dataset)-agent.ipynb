{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894c1cfc-fcc6-468a-bf79-e7b8ae7cf1be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install wikipedia==1.4.0 google-search-results==2.4.2 better-profanity==0.7.0 sqlalchemy==2.0.15 huggingface_hub transformers datasets langchain openai openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff2eb8d1-abb2-4d98-b7db-3944203ca224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate API tokens\n",
    "For many of the services that we'll using in the notebook, we'll need some API keys. Follow the instructions below to generate your own. \n",
    "\n",
    "### Hugging Face Hub\n",
    "1. Go to this [Inference API page](https://huggingface.co/inference-api) and click \"Sign Up\" on the top right.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_sign_up.png\" width=700>\n",
    "\n",
    "2. Once you have signed up and confirmed your email address, click on your user icon on the top right and click the `Settings` button. \n",
    "\n",
    "3. Navigate to the `Access Token` tab and copy your token. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_token_page.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7303ab-b83c-4895-9aef-b96d62c860c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SerpApi\n",
    "\n",
    "1. Go to this [page](https://serpapi.com/search-api) and click \"Register\" on the top right. \n",
    "<img src=\"https://files.training.databricks.com/images/llm/serp_register.png\" width=800>\n",
    "\n",
    "2. After registration, navigate to your dashboard and `API Key` tab. Copy your API key. \n",
    "<img src=\"https://files.training.databricks.com/images/llm/serp_api.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a2147c-b5a1-4bda-9b9e-1ee2434843e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Copy paste your tokens below\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<enter-your-huggingface-token>\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"<enter-your-google-serpapi-token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bead245-780a-4152-82d2-b1c39ebfa9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### OPTIONAL: Use OpenAI's language model\n",
    "\n",
    "If you'd rather use OpenAI, you need to generate an OpenAI key. \n",
    "\n",
    "Steps:\n",
    "1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n",
    "2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "Note: OpenAI does not have a free option, but it gives you $5 as credit. Once you have exhausted your $5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). \n",
    "\n",
    "**IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae208b2-cfa0-4d23-b902-4af8c0b3726b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "#OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d52f900-34b5-4a06-b180-0d2f6ea95bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Command Agent` - Our first vector database data science AI agent!\n",
    "\n",
    "In this section we're going to build an Agent based on the [ReAct paradigm](https://react-lm.github.io/) (or though-action-observation loop) that will take instructions in plain text and perform data science analysis on data that we've stored in a vector database. The agent type we'll use is using zero-shot learning, which takes in the prompt and leverages the underlying LLMs' zero-shot abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd614880-732d-4631-a9a0-005c7a22fd76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For DaScie we need to load in some tools for it to use, as well as an LLM for the brain/reasoning\n",
    "from langchain.agents import load_tools  # This will allow us to load tools we need\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import (\n",
    "    AgentType,\n",
    ")  # We will be using the type: ZERO_SHOT_REACT_DESCRIPTION which is standard\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_base=\"<enter-your-azure-openai-api-endpoint>\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    deployment_name=\"<enter-your-deployment-name\", # This is the name of the gpt-35-turbo deployment you created in Azure (version 0331)\n",
    "    openai_api_key=\"<enter-your-azure-openai-api-key>\",\n",
    "    openai_api_type=\"azure\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"python_repl\", \"terminal\"], llm=llm)\n",
    "# We now create agent executor using the \"initialize_agent\" command.\n",
    "command_agent = initialize_agent(\n",
    "    tools, llm, verbose=True, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, early_stopping_method=\"generate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_agent.run(\"Generate 40 records of a call center FAQ for an ecommerce clothing store. Include products in these categories: designer, shoes, watches, jewelry, hand bags, clothes etc.  Store the customer utterance in the Question column and the store agent in the Answer column. Save the data in a spreadsheet in the local directory. DO NO REPEAT the questions!  Do not include the agent's name!  DO NOT GIVE a blank answer! DO NOT PROVIDE THE SAME ANSWER FOR DIFFERENT QUESTIONS! DO NOT REPEAT A QUESTION!! DO NOT REPEAT SAME TOPIC IN QUESTIONS!  ANSWER MUST ALWAYS BE RELEVANT TO THE QUESTION! LIMIT ANSWERS TO 50 WORDS! LIMIT EXAMPLES IN RESPONSES TO 3 OR LESS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Courtesy of Edx.org LLM & Databricks course</i>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM 03 - Building LLM (dataset)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
